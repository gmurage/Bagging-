{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36bffa9-0655-44b3-abe0-2d16831fb644",
   "metadata": {},
   "source": [
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "   \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "#                                             Bagged Model in Predicting Obesity Risk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##                                                        Gladys Murage\n",
    "\n",
    "##                              College of Business, Engineering, and  Technology, National University\n",
    "\n",
    "##                                         DDS8555 v1: PREDICTIVE ANALYSIS(3602869492)\n",
    "\n",
    "##                                                        Dr MOHAMED NABEEL\n",
    "\n",
    "##                                                            April 15, 2025\n",
    "\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ec8d1-043f-4371-9a27-4f9bf4b7486a",
   "metadata": {},
   "source": [
    "# Key Components Explained:\n",
    "### Base Estimator:\n",
    "The model uses the same Decision Tree parameters from the previous model\n",
    "\n",
    "### Bagging Parameters:\n",
    "1. n_estimators=50. Creates 50 decision trees\n",
    "2. max_samples=0.8. Each tree uses 80% random sample of training data\n",
    "3. max_features=0.8. Each tree uses 80% random features\n",
    "\n",
    "### Advantages Over Single Decision Tree:\n",
    "1. Reduces variance (overfitting)\n",
    "2. Typically provides better generalization\n",
    "3. Handles class imbalance better through random sampling\n",
    "\n",
    "### Output Includes:\n",
    "1. Validation accuracy and full classification report\n",
    "2. Feature importance analysis (averaged across all trees)\n",
    "3. Test predictions in submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c49bc77-3754-4ef3-8da6-5bd825f201c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8572\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       524\n",
      "           1       0.84      0.80      0.82       626\n",
      "           2       0.86      0.77      0.81       543\n",
      "           3       0.97      0.96      0.96       657\n",
      "           4       1.00      1.00      1.00       804\n",
      "           5       0.63      0.78      0.70       484\n",
      "           6       0.72      0.68      0.70       514\n",
      "\n",
      "    accuracy                           0.86      4152\n",
      "   macro avg       0.85      0.84      0.84      4152\n",
      "weighted avg       0.86      0.86      0.86      4152\n",
      "\n",
      "\n",
      "Test predictions saved to 'bagging_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('Otrain.csv')\n",
    "test = pd.read_csv('Otest.csv')\n",
    "sample_sub = pd.read_csv('Osample_submission.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop('NObeyesdad', axis=1)\n",
    "y = train['NObeyesdad']\n",
    "\n",
    "# Preprocessing: Convert categorical variables to numerical\n",
    "ordinal_encoders = {}\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X[column] = oe.fit_transform(X[[column]]).flatten()\n",
    "    ordinal_encoders[column] = oe\n",
    "\n",
    "# Encode target variable if categorical\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Bagging classifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(\n",
    "        max_depth=5,\n",
    "        min_samples_split=10,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_estimators=50,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = bagging_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Prepare test set predictions\n",
    "if not test.empty:\n",
    "    # Apply the same preprocessing to test data\n",
    "    for column in test.select_dtypes(include=['object']).columns:\n",
    "        if column in ordinal_encoders:\n",
    "            test[column] = ordinal_encoders[column].transform(test[[column]]).flatten()\n",
    "    \n",
    "    # Make predictions\n",
    "    test_pred = bagging_model.predict(test)\n",
    "    \n",
    "    # If we encoded the target, inverse transform to get original labels\n",
    "    if 'le_target' in locals():\n",
    "        test_pred = le_target.inverse_transform(test_pred)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = sample_sub.copy()\n",
    "    submission['NObeyesdad'] = test_pred\n",
    "    submission.to_csv('bagging_submission.csv', index=False)\n",
    "    print(\"\\nTest predictions saved to 'bagging_submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d0011-f4db-49e4-963d-48388c00ba9f",
   "metadata": {},
   "source": [
    "# Interpretation of the Bagging Model\n",
    "## Validation Accuracy: 0.8572\n",
    "Performance Improvement. This indicates a better model accuracy compared to the decision tree model's previous result (83.69%). It's a positive step forward, suggesting that the adjustments made by ensemble techniques, like bagging, are working effectively. The weighted F1-score of 0.86 confirms good overall performance across all classes.\n",
    "\n",
    "## Detailed Classification Analysis\n",
    "### Class 0\n",
    "Precision and  Recall both remain at 0.92, showing consistent high performance. F1-Score of 0.92 reflects a perfect balance between precision and recall. Class 0 remains a strong performer, with no significant concerns.\n",
    "\n",
    "### Class 1:\n",
    "Precision shows 84% accuracy in predictions for Class 1. Recall is improved to 80%, indicating better identification of true instances within class 1. F1-Scoreof 0.82 now balances precision and recall more effectively. The higher recall suggests reduced false negatives, which is a positive step for this class.\n",
    "\n",
    "### Class 2:\n",
    "Precision is 86%, which is slightly higher than before in the decision tree. Recall remains at 77%, showing room for improvement in true positive identification. The bagging model only identifies correct instances 77% of the time and 33% of the time gets it wrong. F1-Score of 0.81 demonstrates solid overall performance. Class 2 continues to perform well, though a focus on recall optimization might further enhance results.\n",
    "\n",
    "### Class 3:\n",
    "Precision and  Recall are both near perfection at 0.97 and 0.96, respectively. Meaning that bagging gets the class classified accurately 97% of the time and gets the instances correct 96% of the time. An F1-Scoreof 0.96 remains stellar. Class 3 is evidently well-separated and easy to classify.\n",
    "\n",
    "### Class 4:\n",
    "Precision, Recall, and  F1-Score all remain at 1.00, reflecting flawless performance. This class benefits from highly distinct features, requiring little to no further adjustments.\n",
    "\n",
    "### Class 5:\n",
    "Precision is improved to 63%, showing better prediction accuracy but the model is still mis-classifying 37% of this class. Recall remains high at 78%, reflecting strong identification of true instances when the model classifies. F1-Score now at 0.70, indicates improved balance. While precision has improved, further work is still needed to address false positives in this class.\n",
    "\n",
    "### Class 6:\n",
    "Precision, Recall, and F1-Score metrics are consistent with the previous results, hovering around 0.70. Class 6 remains steady but could benefit from focused tuning to enhance both precision and recall.\n",
    "\n",
    "## Macro and Weighted Averages\n",
    "Macro Average (0.85). Shows improved overall balance between precision and recall across all classes. Weighted Average (0.86). Slightly higher than before in the decision tree, reflecting better performance for larger classes like 4 and 3.\n",
    "\n",
    "## Insights on Bagging\n",
    "Bagging is an ensemble technique and unlike decision trees, bagging helps reduce overfitting and variance, which may explain the improved accuracy and precision for several classes. However, Class 5 and Class 6 still seem to struggle comparatively, likely due to overlapping features or insufficient representation in the dataset.\n",
    "\n",
    "## Key Observations\n",
    "### Class Imbalance Impact:\n",
    "1. Larger classes (3,4) perform better than smaller ones (5,6). Weighted avg (0.86) > macro avg (0.84) confirms this\n",
    "\n",
    "### Bagging Benefits:\n",
    "1. 2% accuracy boost over single Decision Tree\n",
    "2. More balanced recall across classes\n",
    "3. Better handling of Class 1 (recall improved from 0.65 to 0.80)\n",
    "\n",
    "### Persistent Challenges:\n",
    "1. Class 5 still has precision issues with many false positives.\n",
    "2. Class 6 remains problematic with both precision and recall < 0.75.\n",
    "\n",
    "## Recommendations for Improvement\n",
    "1. For class 5's low precision increase weight\n",
    "2. For class 6's general weakness consider feature engineering for distinguishing characteristics\n",
    "3. For the entire bagging model, consider trying random forest for better performance.\n",
    "4. Advanced techniques such as hyper-parameter tuning and Grid search may help improve results\n",
    "\n",
    "## Business Implications\n",
    "### Reliable Predictions:\n",
    "The bagging model can confidently predict Classes 0, 3, and 4. Use cases requiring these predictions are production-ready\n",
    "\n",
    "### Caution Areas:\n",
    "Applications relying on Class 5 or 6 predictions need human verification. Consider collecting more data for these classes\n",
    "\n",
    "## Next Steps:\n",
    "The 85.7% accuracy may be sufficient for many applications.,For critical use cases, consider stacking with other models\n",
    "\n",
    "This model represents a significant improvement over the initial Decision Tree, particularly in handling class imbalance and improving recall for problematic classes. The remaining challenges with Classes 5 and 6 are likely due to either:\n",
    "1. Insufficient distinguishing features in the data\n",
    "2. Genuine ambiguity in these categories\n",
    "3. Need for more sophisticated modeling techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
